{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "\n# ## The first step is to transform all of the .hdf5 files into a usable dataframe. This first cell iterates over the directory and places the data into a Dask dataframe.\n# \n\n# In[1]:\n\n\nimport time\nimport dask.dataframe as dd\nimport os\nimport sys\nimport time\nimport glob\nimport datetime\nimport sqlite3\nimport numpy as np \nimport pandas as pd\n# we define this very useful function to iterate the files\ndef list_all_files(filelist,basedir,func=lambda x: x,ext='.h5'):\n    \"\"\"\n    From a base directory, go through all subdirectories,\n    find all files with the given extension, apply the\n    given function 'func' to all of them.\n    If no 'func' is passed, we do nothing except counting.\n    INPUT\n       basedir  - base directory of the dataset\n       func     - function to apply to all filenames\n       ext      - extension, .h5 by default\n    RETURN\n       number of files\n    \"\"\"\n    cnt = 0\n    # iterate over all files in all subdirectories\n    for root, dirs, files in os.walk(basedir):\n        files = glob.glob(os.path.join(root,'*'+ext))\n        # add files to list\n        filelist.extend(files)\n             \n    return filelist\n\n\nstart = time.time()\nmylist = list()\nsongdir = 'pathto/MillionSongSubset' # to run this locally, you must download\nsonglist = list_all_files(mylist, songdir)\ndel songlist[:3]\nprint(songlist)\ndf = dd.read_hdf(songlist,'/metadata/songs')\ndf2 = dd.read_hdf(songlist, '/analysis/songs')\ndf4 = dd.read_hdf(songlist, '/musicbrainz/songs')\ndf3 = dd.multi.concat([df,df2,df4], axis=1)\ndf3.head()\nend = time.time()\n\n\n## Time - 2.5 minutes - scale to MSD... ~ 4 hours. Not bad\n\n\nstart = time.time()\ndf =df3.compute()\nend = time.time()\nprint(end-start)\n#time - 7.5 minutes - Scale to MSD... ~ 12 hours. Wish this was faster.\n\n\n# ## Now, we have to use Spotipy to fill in some missing audio analysis features.\n\nimport spotipy\nfrom spotipy.oauth2 import SpotifyClientCredentials\ncid = 'your_client_id'\nsecret = 'your_client_secret'\nclient_credentials_manager = SpotifyClientCredentials(client_id=cid, client_secret=secret)\nsp = spotipy.Spotify(client_credentials_manager\n=\nclient_credentials_manager)\n\n\nimport time\ndef get_sp_track_id(this_track):\n    artist = this_track['artist_name']\n    song = this_track['title']\n    try:\n        track_id = sp.search(q='artist:' + artist + ' track:' + song,type='track')\n        return track_id['tracks']['items'][0]['id']\n    except:\n        print('no such track found')\n        return None\nstart_time = time.time()\n\n# Apply the function to each row of the DataFrame\nsp_track_ids = []\nfor i, row in df.iterrows():\n    sp_track_id = get_sp_track_id(row)\n    sp_track_ids.append(sp_track_id)\ndf['sp_track_id'] = sp_track_ids\n\nend_time = time.time()\n\nprint(\"Time elapsed: \", end_time - start_time, \"seconds\")\n#time - 24 minutes - Scale to MSD... ~ 40 hours T_T\n\n\ndropped_df = df.drop(['danceability','energy','loudness','mode','tempo','key','key_confidence','time_signature'],axis=1)\ndropped_df = dropped_df[~dropped_df['sp_track_id'].isna()]\nimport numpy as np\nimport time\nstart = time.time()\nfeature_list = ['danceability','energy','key','loudness','mode','speechiness','acousticness','instrumentalness','liveness','valence','tempo','time_signature']\nfeatures_list = []\ntrack_ids = dropped_df['sp_track_id'] #column of all track_ids\nfor x in track_ids: # for each track id\n    results = sp.audio_features(x) #get its audio features\n    feature_values = [] #make an empty list\n    try:\n        for y in feature_list: #for each of the features i want\n            feature_values.append(results[0][y]) #add the key value to the empty list\n    except:\n        print(\"No audio features for this track\")\n    features_list.append(feature_values)\nfeatures_df = pd.DataFrame(features_list,columns= feature_list)\n\nend = time.time()\nprint(end-start)\n# Time -  11 minutes, scaled to MSD... ~ 18 hours\n\n#Total time for all operations - \n\n\nfull_df = pd.merge(features_df, dropped_df, on=dropped_df.index)\n\n\nfull_df.head()\n\nimport pandas as pd\ndf = pd.read_csv('pathto/finished_dataset.csv')\nstyles = pd.read_csv('pathto/Styles (1).csv',header=None)\n\nstyles.rename(columns = {0:'track_id'}, inplace = True)\nstyles.rename(columns = {1:'Style'},inplace=True)\nstyles.head()\ndf_merged = pd.merge(df,styles, on = 'track_id',how='left').fillna(0)\ndf_merged\n\n\npd.DataFrame(df_merged.Style.unique())\n\n\n#Let's set up a smaller df and preprocess it for a random forest genre classification model.\nimport numpy as np\ndf_with_genre = df_merged[df_merged['Style']!=0]\ndf_with_genre = df_with_genre[['title','artist_name','track_id','song_id','sp_track_id','Style','danceability','energy','key','loudness','speechiness','acousticness','instrumentalness','valence','tempo','time_signature','year']]\n\n\ndf_with_genre['year'].replace(0, df_with_genre['year'].median(), inplace=True)\ndf_with_genre['key'].replace(0, df_with_genre['key'].median(), inplace=True)\ndf_with_genre['instrumentalness'].replace(0, df_with_genre['instrumentalness'].median(), inplace=True)\ndf_with_genre.replace(0, np.nan, inplace=True)\ndf_rf_dropped=df_with_genre.dropna()\nkMeansFrame = df_rf_dropped[['track_id','Style','danceability','energy','key','loudness','speechiness','acousticness','instrumentalness','valence','tempo','time_signature','year']]\n\n\ndf_rf_dropped.head()\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\nfrom scipy.stats import randint\n\nx = df_rf_dropped.drop(['Style','title','artist_name','track_id','song_id','sp_track_id'],axis=1)\ny = df_rf_dropped['Style']\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2)\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n\n##this is pretty bad.. maybe it is not possible to detect genre from audio features like this. Let's try clustering instead, that may provide some better recommendations.\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import OrdinalEncoder,LabelEncoder # for encoding categorical features from strings to number arrays\n\nkm = KMeans(n_clusters=10, random_state=123)\nle = LabelEncoder()\nkMeansFrame.Style = le.fit_transform(kMeansFrame.Style)\n#normalize the data\nkMeansNorm = kMeansFrame.drop('track_id',axis=1)\nfor col in kMeansNorm.columns:\n    kMeansNorm[col] = kMeansNorm[col]  / kMeansNorm[col].abs().max()\ndisplay(kMeansNorm)\n\n#Let's tune our model\nimport matplotlib.pyplot as plt\nwcss = [0] * 23\nfor i in range(2,25):\n    kModel = KMeans(n_clusters=i, random_state=123)\n    kModel.fit_predict(kMeansNorm)\n    wcss[i-2]=kModel.inertia_\nx = list(range(2,25))\nplt.plot(x,wcss)\nkModel = KMeans(n_clusters=10, random_state=123)\nclusters = kModel.fit_predict(kMeansNorm)\nkMeansFrame['cluster'] = clusters\nkMeansFrame\n\n\n#Looks like 10 is good, so we will keep it at that.\ndf_with_clusters = pd.merge(df_merged,kMeansFrame[['track_id','cluster']], on = 'track_id',how='left')\ndf_with_clusters\n\n\nuserScores = pd.read_csv('pathto/UserScores.csv',header=None)\n#rename columns\nuserScores.rename(columns = {0:'user_id'}, inplace = True)\nuserScores.rename(columns = {1:'song_id'},inplace=True)\nuserScores.rename(columns = {2:'plays'},inplace=True)\nuserScores.head()\n#get array of unique users\nuniqueUser = pd.DataFrame(userScores.user_id.unique())\nuniqueSongs = pd.DataFrame(userScores.song_id.unique())\nuniqueUser\nuniqueUser.rename(columns = {0:'user_id'}, inplace = True)\nuniqueSongs.rename(columns = {0:'song_id'}, inplace = True)\n\n\nsubsetUserList = pd.DataFrame(userScores['user_id'].value_counts(ascending=True))\nsubsetUserList_over20 = subsetUserList[subsetUserList['user_id']>=20]\nsubsetUserList_over20.rename(columns = {'user_id':'count'}, inplace = True)\nsubsetUserList_over20['user_id']=subsetUserList_over20.index\n\n\nsubsetUserList_over20\n\n\nsubsetUniqueUsers = uniqueUser[uniqueUser['user_id'].isin(subsetUserList_over20['user_id'])]\nsubsetUniqueUsers\nsubsetUserScores =  userScores[userScores['user_id'].isin(subsetUserList_over20['user_id'])]\nsubsetUserScores\n\n\nuserMatrix = subsetUserScores.pivot_table(index='user_id', columns='song_id', values='plays',fill_value = 0)\n\n\nfrom scipy.sparse import csr_matrix\nfrom sklearn.neighbors import NearestNeighbors\nkNNMatrix = csr_matrix(userMatrix.values)\nmodel_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')\nmodel_knn.fit(kNNMatrix)\n\n\ncollab_songRecFrame = pd.DataFrame(subsetUniqueUsers['user_id'])\ndistances, indices = model_knn.kneighbors(userMatrix.iloc[0,:].values.reshape(1,-1), n_neighbors=6)\nnearNeighbors_idx = pd.DataFrame(indices)\nfor i, user_indices in enumerate(indices):\n    print(f\"User {i} nearest neighbors:\", user_indices)\nnearNeighbors_idx.drop(columns=nearNeighbors_idx.columns[0], axis=1,  inplace=True)\nnn_idx_t = nearNeighbors_idx.T\nnn_idx_t.rename(columns = {0:'user_idx'}, inplace = True)\n\nsr = []\n# for idx in nn_idx_t\nfor i in nn_idx_t['user_idx']:\n    nn1 = userScores[userScores['user_id']==userMatrix.iloc[i,:].name]\n    user = userScores[userScores['user_id']==userMatrix.iloc[0,:].name]\n    nn_novel = nn1[~nn1['song_id'].isin(user['song_id'])]\n    nn_novel.sort_values('plays',ascending=False)\n    sr.append(nn_novel['song_id'].head(1))\n\nsr\n\n\nall_sr = []\nuserlist= []\nimport time\nstart = time.time()\nfor user in range(0,10):\n    \n    distances, indices = model_knn.kneighbors(userMatrix.iloc[user,:].values.reshape(1,-1), n_neighbors=6)# get indices for 6 NN\n    nearNeighbors_idx = pd.DataFrame(indices) #make a dataframe list of the indices\n    nearNeighbors_idx.drop(columns=nearNeighbors_idx.columns[0], axis=1,  inplace=True) #drop the first one (user themselves)\n    nn_idx_t = nearNeighbors_idx.T #transpose\n    nn_idx_t.rename(columns = {0:'user_idx'}, inplace = True) #rename column\n    sr = []\n    for i in nn_idx_t['user_idx']: #for 5 nearest users\n        nn1 = userScores[userScores['user_id']==userMatrix.iloc[i,:].name] # get rows from userScores at index i\n        user1 = userScores[userScores['user_id']==userMatrix.iloc[user,:].name] # get rows from userScores at \n        nn_novel = nn1[~nn1['song_id'].isin(user1['song_id'])]\n        nn_novel.sort_values('plays',ascending=False)\n        sr.append(nn_novel['song_id'].iloc[0])\n    all_sr.append(sr)\n    userlist.append(userMatrix.iloc[user,:].name)\nend= time.time()\nprint(end-start)\n\nsongRecs = pd.DataFrame(all_sr)\nsongRecs['users'] = userlist\nsongRecs['song0'] = df_rf_dropped[df_rf_dropped['song_id'].isin(songRecs[0])].title\nsongRecs['song1'] = df_rf_dropped[df_rf_dropped['song_id'].isin(songRecs[1])].title\nsongRecs['song2'] = df_rf_dropped[df_rf_dropped['song_id'].isin(songRecs[2])].title\nsongRecs['song3'] = df_rf_dropped[df_rf_dropped['song_id'].isin(songRecs[3])].title\nsongRecs['song4'] = df_rf_dropped[df_rf_dropped['song_id'].isin(songRecs[4])].title\n#df_match = df_rf_dropped.drop(df_rf_dropped['song_id'].isin(songRecs[0]))\nsongRecs",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}